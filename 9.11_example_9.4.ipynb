{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32eaa461",
   "metadata": {},
   "source": [
    "To see the potential benefits of using interest and emphasis, consider the four-state\n",
    "Markov reward process shown below:\n",
    "\n",
    "+1     +1     +1     +1  \n",
    "vπ = 4  vπ = 3  vπ = 2  vπ = 1  \n",
    "i = 1  i = 0  i = 0  i = 0  \n",
    "w₁     w₁     w₂     w₂  \n",
    "\n",
    "Episodes start in the leftmost state, then transition one state to the right, with a\n",
    "reward of +1 on each step until the terminal state is reached. The true value of\n",
    "the first state is thus 4, of the second state 3, and so on as shown below each state.\n",
    "These are the true values; the estimated values can only approximate these because\n",
    "they are constrained by the parameterization. There are two components to the\n",
    "parameter vector **w = (w₁, w₂)ᵀ**, and the parameterization is as written inside\n",
    "each state. The estimated values of the first two states are given by **w₁** alone and\n",
    "thus must be the same even though their true values are different. Similarly, the\n",
    "estimated values of the third and fourth states are given by **w₂** alone and must be\n",
    "the same even though their true values are different. Suppose that we are interested\n",
    "in accurately valuing only the leftmost state; we assign it an interest of 1 while all\n",
    "the other states are assigned an interest of 0, as indicated above the states.\n",
    "\n",
    "---\n",
    "\n",
    "First consider applying **gradient Monte Carlo algorithms** to this problem. The\n",
    "algorithms presented earlier in this chapter that do not take into account interest\n",
    "and emphasis (in (9.7) and the box on page 202) will converge (for decreasing step\n",
    "sizes) to the parameter vector **w₁ = (3.5, 1.5)**, which gives the first state—the only\n",
    "one we are interested in—a value of 3.5 (i.e., intermediate between the true values\n",
    "of the first and second states). The methods presented in this section that do use\n",
    "interest and emphasis, on the other hand, will learn the value of the first state\n",
    "exactly correctly; **w₁** will converge to 4 while **w₂** will never be updated because the\n",
    "emphasis is zero in all states save the leftmost.\n",
    "\n",
    "---\n",
    "\n",
    "Now consider applying **two-step semi-gradient TD methods**. The methods from\n",
    "earlier in this chapter without interest and emphasis (in (9.15) and (9.16) and\n",
    "the box on page 209) will again converge to **w₁ = (3.5, 1.5)**, while the methods\n",
    "with interest and emphasis converge to **w₁ = (4, 2)**. The latter produces the\n",
    "exactly correct values for the first state and for the third state (which the first state\n",
    "bootstraps from) while never making any updates corresponding to the second or\n",
    "fourth states.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4865ec98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.99981931 0.        ]\n",
      "[3.999819306616098, 3.999819306616098, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "states = [0, 1, 2, 3, 4]\n",
    "interest = [1, 0, 0, 0, 0]\n",
    "start_state = 0\n",
    "terminal_states = [4]\n",
    "\n",
    "TRUE_VALUES = [4, 3, 2, 1, 0]\n",
    "\n",
    "def get_reward_and_next_state(state, action='right'):\n",
    "    if action == 'right':\n",
    "        if state == 4:\n",
    "            return 0, 4\n",
    "        else:\n",
    "            return 1, state + 1\n",
    "    elif action == 'left':\n",
    "        if state == 0:\n",
    "            return 0, 0\n",
    "        else:\n",
    "            return 1, state - 1\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid action: {action}\")\n",
    "\n",
    "def feature_vector(state):\n",
    "    if state == 4:\n",
    "        return np.array([0.0, 0.0], dtype=np.float64)\n",
    "    if state == 0 or state == 1:\n",
    "        return np.array([1.0, 0.0], dtype=np.float64)\n",
    "    if state == 2 or state == 3:\n",
    "        return np.array([0.0, 1.0], dtype=np.float64)\n",
    "    raise ValueError(f\"Invalid state: {state}\")\n",
    "\n",
    "def value_function(w, state):\n",
    "    if state in terminal_states:\n",
    "        return 0.0\n",
    "    return float(np.dot(w, feature_vector(state)))\n",
    "\n",
    "def generate_episode():\n",
    "    state = start_state\n",
    "    episode = []\n",
    "    while state not in terminal_states:\n",
    "        reward, next_state = get_reward_and_next_state(state, 'right')\n",
    "        episode.append((state, reward))\n",
    "        state = next_state\n",
    "    return episode\n",
    "\n",
    "def gradient_mc_update(w, num_episodes, alpha, gamma=1.0):\n",
    "    for _ in range(num_episodes):\n",
    "        episode = generate_episode()  # list of (state, reward)\n",
    "        G = 0.0\n",
    "        M = 0.0\n",
    "        for i, (state, reward) in enumerate(reversed(episode)):\n",
    "            G = reward + gamma * G\n",
    "            v_hat = value_function(w, state)\n",
    "            grad = feature_vector(state)\n",
    "            w += alpha * (G - v_hat) * grad * interest[state]\n",
    "    return w\n",
    "\n",
    "\n",
    "w = np.zeros(2, dtype=np.float64)\n",
    "w = gradient_mc_update(w, 10000, 0.001)\n",
    "print(w)\n",
    "estimates = [value_function(w, s) for s in states]\n",
    "print(estimates)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
